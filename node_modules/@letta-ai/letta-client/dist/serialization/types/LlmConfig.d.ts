/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as serializers from "../index";
import * as Letta from "../../api/index";
import * as core from "../../core";
import { LlmConfigModelEndpointType } from "./LlmConfigModelEndpointType";
import { ProviderCategory } from "./ProviderCategory";
import { LlmConfigReasoningEffort } from "./LlmConfigReasoningEffort";
export declare const LlmConfig: core.serialization.ObjectSchema<serializers.LlmConfig.Raw, Letta.LlmConfig>;
export declare namespace LlmConfig {
    interface Raw {
        model: string;
        model_endpoint_type: LlmConfigModelEndpointType.Raw;
        model_endpoint?: string | null;
        provider_name?: string | null;
        provider_category?: ProviderCategory.Raw | null;
        model_wrapper?: string | null;
        context_window: number;
        put_inner_thoughts_in_kwargs?: boolean | null;
        handle?: string | null;
        temperature?: number | null;
        max_tokens?: number | null;
        enable_reasoner?: boolean | null;
        reasoning_effort?: LlmConfigReasoningEffort.Raw | null;
        max_reasoning_tokens?: number | null;
    }
}
