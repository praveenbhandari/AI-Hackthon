/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as Letta from "../index";
/**
 * Configuration for a Language Model (LLM) model. This object specifies all the information necessary to access an LLM model to usage with Letta, except for secret keys.
 *
 * Attributes:
 *     model (str): The name of the LLM model.
 *     model_endpoint_type (str): The endpoint type for the model.
 *     model_endpoint (str): The endpoint for the model.
 *     model_wrapper (str): The wrapper for the model. This is used to wrap additional text around the input/output of the model. This is useful for text-to-text completions, such as the Completions API in OpenAI.
 *     context_window (int): The context window size for the model.
 *     put_inner_thoughts_in_kwargs (bool): Puts `inner_thoughts` as a kwarg in the function call if this is set to True. This helps with function calling performance and also the generation of inner thoughts.
 *     temperature (float): The temperature to use when generating text with the model. A higher temperature will result in more random text.
 *     max_tokens (int): The maximum number of tokens to generate.
 */
export interface LlmConfig {
    /** LLM model name. */
    model: string;
    /** The endpoint type for the model. */
    modelEndpointType: Letta.LlmConfigModelEndpointType;
    /** The endpoint for the model. */
    modelEndpoint?: string;
    /** The provider name for the model. */
    providerName?: string;
    /** The provider category for the model. */
    providerCategory?: Letta.ProviderCategory;
    /** The wrapper for the model. */
    modelWrapper?: string;
    /** The context window size for the model. */
    contextWindow: number;
    /** Puts 'inner_thoughts' as a kwarg in the function call if this is set to True. This helps with function calling performance and also the generation of inner thoughts. */
    putInnerThoughtsInKwargs?: boolean;
    /** The handle for this config, in the format provider/model-name. */
    handle?: string;
    /** The temperature to use when generating text with the model. A higher temperature will result in more random text. */
    temperature?: number;
    /** The maximum number of tokens to generate. If not set, the model will use its default value. */
    maxTokens?: number;
    /** Whether or not the model should use extended thinking if it is a 'reasoning' style model */
    enableReasoner?: boolean;
    /** The reasoning effort to use when generating text reasoning models */
    reasoningEffort?: Letta.LlmConfigReasoningEffort;
    /** Configurable thinking budget for extended thinking, only used if enable_reasoner is True. Minimum value is 1024. */
    maxReasoningTokens?: number;
}
